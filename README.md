

# ğŸ¤– Zero-Shot LLM Task Pipeline

Built an end-to-end **Zero-Shot Learning Text Inference App** using **Streamlit** and **OpenAI GPT models (GPT-3.5-turbo / GPT-4)**.

This application enables users to dynamically perform multiple natural language tasks â€” **Summarization**, **Translation**, **Question Generation**, and **Sentiment Analysis** â€” by simply providing input text without any additional training.

---

## ğŸš€ Live Demo
ğŸ‘‰ [Visit Live App Here](https://zero-shot-llm-pipeline-s7qsz4b4chypyjrhrcapprk.streamlit.app)

---

## ğŸ“š Features

- ğŸ”¹ **Dynamic Zero-shot Prompting** for various NLP tasks
- ğŸ”¹ **Model Switching** (choose between GPT-3.5-turbo and GPT-4)
- ğŸ”¹ **Task-specific Example Auto-fill** to make testing faster
- ğŸ”¹ **Real-time Response Time Measurement**
- ğŸ”¹ **Approximate Token Usage Estimation**
- ğŸ”¹ **Clean UI** with Spinner Loading and Input Clearing
- ğŸ”¹ **Deployed** on **Streamlit Community Cloud**

---

## ğŸ› ï¸ Tech Stack

- **Frontend:** Streamlit
- **LLM API:** OpenAI GPT-3.5-turbo / GPT-4
- **Prompt Engineering:** Zero-shot Dynamic Prompt Templates
- **Deployment:** Streamlit Community Cloud
- **Version Control:** Git, GitHub

---

## ğŸ–¥ï¸ How It Works

1. Choose a **task** (Summarization, Translation, Question Generation, Sentiment Analysis)
2. Enter your **text** or auto-fill from provided examples
3. Select the **model** (GPT-3.5 or GPT-4)
4. Run the task and view:
   - Generated output
   - Selected model and task details
   - Response time
   - Approximate token usage

---

## ğŸ“¦ Local Setup Instructions

Want to run this project locally?  
Follow these steps:

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/zero-shot-llm-pipeline.git
cd zero-shot-llm-pipeline
```

### 2. Create a Virtual Environment (Recommended)

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Set Environment Variable

Create a `.env` file inside the root directory:

```plaintext
OPENAI_API_KEY=your-openai-api-key-here
```

(Or set the OpenAI API key manually inside the code for testing.)

### 5. Run Streamlit App

```bash
streamlit run src/app_streamlit.py
```

âœ… Your app will launch at `http://localhost:8501`.

---

## âœ¨ Future Enhancements

- Add **Few-Shot Learning** capabilities (example-augmented prompts)
- Deploy **backend FastAPI server** separately for scalable architectures
- Integrate **multiple LLM providers** (Anthropic Claude, Google Gemini, etc.)
- Implement **user authentication** for managing API costs

---

## ğŸ“„ License

This project is licensed under the MIT License - feel free to modify and use it!

---

# ğŸ“¢ Credits

Special thanks to the open-source community and OpenAI APIs for powering the backend intelligence behind this project!

Built with ğŸ’», ğŸš€, and a lot of â˜•ï¸.

---

# ğŸ¯ Project Snapshot

| Component | Status |
|:----------|:-------|
| Working Frontend (Streamlit) | âœ… |
| LLM Model Switching (3.5/4.0) | âœ… |
| Deployment to Streamlit Cloud | âœ… |
| Token Usage Tracking | âœ… |
| Resume-Portfolio Ready | ğŸ”¥ |

---

# ğŸ”¥ Final Note:

If you like this project, feel free to â­ï¸ the repository and connect!

[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue)](https://www.linkedin.com/in/your-linkedin-profile)

---
